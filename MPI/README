1. MPI

P to P:
comutator1     comutator2
  rank1		     rank5
  rank2			 rank6
  rank3			 rank7
  rank4			 rank8
  ...			 ...

send/recv: http://mpitutorial.com/mpi-send-and-receive/
	point-to-point : 

dynamic data [comunication]:

get_count:
probe:
	http://mpitutorial.com/dynamic-receiving-with-mpi-probe-and-mpi-status/

parallelization random walker app:
	http://mpitutorial.com/dynamic-receiving-with-mpi-probe-and-mpi-status/
	[domains, walkers,status]

	program pattern!! -- The Essential of Programming [Patterns]

	1. Initialize the walkers.
	2. Progress the walkers with the walk function.
	3. Send out any walkers in the outgoing_walkers vector.
	4. Receive new walkers and put them in the incoming_walkers vector.
	5. Repeat steps two through four until all walkers have finished.

	To increase the communication, not decerease. To avoid deadlock by communication.

Collective Communication
	barrier

	"
		Always remember that every collective call you make is synchronized. In other words, if you can’t 
	successfully complete an MPI_Barrier, then you also can’t successfully complete any collective call. If you 
	try to call MPI_Barrier or other collective routines without ensuring all processes in the communicator will 
	also call it, your program will idle. This can be very confusing for beginners, so be careful!
																										"

    " A smarter implementation is a tree-based communication algorithm that can use more of the available 
    network links at once.  "

    wtime
    broadcast : the same data set

    scatter: seperated data set
    gather: This routine is highly useful to many parallel algorithms, such as parallel sorting and searching. 
    allgather







